Lecture 1.1: Data Science and Big Data (17min)

Data is the new oil !
In the last 10 min. we generated more data than from prehistoric times until 2003 !
We are all generating event data : bying a coffee, making a phone call, appointment, ...
Internet of Events = all the data that is being recorded in all kind of ways.

4 different sources of event data
-Internet of content: classical internet that we know: google, wikipedia
-Internet of people: twitter, facebook, social events that generate data
-Internet of things: Another source of event data, device conected to internet, refrigerator, ...
- Internet of places:  The sensors on your phone recording when you are or what you are doing...

Big Data: 
Incredible amounts of event data that are being recorded
Moore's law -> 2sqrt(20) = 1.048.576x in 40 years
The challenge today is not generate more data, the challenge is to turn this data into real value.

4 V's of Big Data:
-Volume: Data size
-Velocity: Speed of change
-Variety: Different forms of data sources
-Veracity: Uncertainty of data

Data does not habe to be "big" to be challenging
data analytics quastion are everywhere, Need for data scientists!
A data scientist is able to collect, analyze and interpret data from a variety of sources 
Turning data into value

4 generic data science questions that you can ask in any situation:
1-What happened?
2-Why did it happen?
3-What will happen?
4-What is the best that can happen?

Data science skills needed to answer such questions
you need to be able to deal with incredibly large databases, have knowledge of social sciences,
think about the value of data, how can extract value from it, apply process mining techniques to
these data, to learn and improve process.

Focus on the analysis on process is based on data, process are key, you don't want to improve data,
you want to improve processes. They are the thing that matter. Not the data, Not the software
It's also different from data mining: not interested in just isolated decisions or low level patterns,
interested in improving end-to-end processes, that's a key thing.

Focus in on the interplay between event data and process
What are the 'highways' in my process?
What factors are influencing a bottleneck?
Can we predict problems(delay, deviation, risk,etc.)for running cases?
Can we recommend countermeasures?
How to redesign the process/organization/machine
...
Process Mining is Data Science in Action, we are looking at the dynamics of machines and
business processes and try to learn from them and improve them
 
           - - - - - - - - - -          - - - - - - - - - - 
 
 Lecture 1.2: Different Types of Process Mining (21min)
 
 3 types mining
 -Process discovery
 -Performance checking
 -Enhancement
 
Process mining is bridging the gap between classical process model analysis and data oriented analysis
like mining and machine learning, process mining is bridging this gap because it's focusing on processes, 
but at the same time, using the real data.
In classical data mining people typically do not look at the processes, especially not end to end processes,
In areas where people are concerned with process model analysis,they typically ignore the data.
We are doing process mining to answer performance related questions and compliance related questions.

The starting point for process mining is event data; An event has different properties, so there are generic
types of properties.

We are focusing on the relationship between process models and event data
3 types of relationships between models and event data:
-play-out
-play-in
-replay

Play-Out: The basic idea is that you start from a model, and from that model, you generate behavior.
If you are using simulation or if you are building an information system that is driven by such models, 
play-out is the thing that you are doing.

Play-In: From even data to the corresponding process model  
The basic idea is you look at the number of example behaviors and you automacally  infer a model from it
We are automatically learning a model from examples.
no modelling is needed, we are not making any models by hand, we automatically infer process models from 
raw even data.

Replay: The key element is that you can replay a reality on top of models, is important you try to replay
reality on top of the model.
We see the rejection, we can record a problem, provide diagnostics, continue our replay; we can see where 
the deviations are.
Replay is not about conformance, it's also about performance analysis
Replaying you can see exactly where delays take place, you can startg investigating what is causing them.

Overwiew
We have a real world where things are happening, we have a software system that is, somehow, recording 
events of the things that take place. People objecting to the valuation of their house. Patients being
treated in a hospital. Then we have this event data and we can do discovery, automatically learning
process models, we can do conformance checking, comparing the event log to the process model and we can
enrich, it's called enhancement, we can enrich the process model with information about deviations and 
performance.
Play-out is about the classical process models not involving any event data
Play-in corresponds to discovery, you automatically learn a process model without any modeling from raw
event data
Replay where you compare a process model to an event log, to check conformance, to investigate performance
problems, etc.

           - - - - - - - - - -          - - - - - - - - - - 

Lecture 1.3: How Process Mining Relates to Data Mining (20min)

Process mining is the missig link between model based analysis, process model based analysis, and data
oriented analysis like data mining, with the goal to answer performance oriented questions and complience
oriented questions.
We can think of process mining as super glue, the glue between data and processes, glue between business
people and IT people, glue business intelligence and business process management, glue between performance
and compliance and you can do this at runtime and at design time, so it's connecting many different things
and that makes it so incredibly valuable.
Process mining consists of differents types of mining and here you can see (graph 01:31) process discovery
and conformance checking, later we will also look at predictive analytics, how you can use process mining
to predict things about the future.

4 different data sets, each consisting of 11 elements(02:18)

One can think of process mining as finding desire lines

analogy1:  the trail can be seen as event data, and the sign saying that you should not walk there (03:43)
that you should use the official route can be seen as the process model. You try to uncover the desire lines, 
what do people really do.

analogy2: (04:11) it is clearly showing that people do not follow the process model, the gate that you see
here is aimed of keeping cyclist out of this park, but what you can see is that the desire line is showing
that people do not follow the things that they should do here, so the gate does not work as it is supposed
to work. 
Using process mining and conformance checking, you can use and show these types of things.

Process mining in action (ProM6) (05:10)

The example a child learning language(08:10) analogic, we can see a sentence and compare that to a trace
in an event log, and we can think of the language as a process model; this is the relationship between
understanding a language based on examples from learning a process model just by looking at the examples.
You want to see how does reality deviate from the modeled process, you can compare that to spell checking,
so the spell checker has a model of the language, and you type a piece of text and then it is checked,
wether that piece of text fits the language, as it has been formalized. This can be compared to the typical
diagnostics that you get when you do conformance checking in the are of process mining, so you see activies
that have happened, but that should not happen, or the other way around, or you will see activities that
were executed too late, or too early, or by the wrong person. This can all be compared by spell checking.

We  focus on the topic of Data Mining, process mining is very different from the classical data mining 
techniques, but are many relationships, so in this course you will also get a basic understanding of
what process mining is all about and of course all data mining.

The growth of the digital universe is driving the fact that many people are using data mining techniques
Initially the term data mining had a very negative connotation, people talked about it as data snooping,
fishing, etc; Statiscians did not consider it the proper way to do, but this is now a very mature
discipline driven by these huge amounts of data, but data mining is data-centric and not process-centric

Variables
Data set(sample or table) consists of instances(individuals, entities, cases, objects or records)
Variables are often referred to as attributes, features or data elements.
We can find 2 differents groups:
1- Categorical Variables:
 -ordinal(high-med-low, cum laude-passed-failed) or
 -nominal(true, false, red-pink-green)
2- Numerical Variables
 -ordered, cannot be enumerated easily
 
2 types of data mining techniques: Supervised and unsupervised learning
- Supervised learning:
We have labeled data, means that there is a response variable that labels the instance.
The goal of supervised learning is to learn from the other variables that are called
predictor variables, what our response variable is going to be, instead of response
variable, we also talk about dependent variables and instead of predictor variables we
also talked about independent variables and the goal is to explain the dependent 
variable in terms of the independent variables.

Classification techniques, like learning decision tree, assume a categorial response variable
and the goal is to classify instances based on the predictor variables.

Regression techniques assume a numerical response variable. The goal is to find a function that
fits the data with the least error.

- Unsupervised learning:
Assumes unlabeled data, i.e., the variables are not split into response and predictor variables.
Examples: Clustering(e.g., k-means clustering and agglomerative hierarchical clustering) and 
pattern discovery(association rules)
 
There are many data mining tools available: RapidMiner, R, Weka, KNIME, SAS, IBM SPSS, IBM Cognos,
QlikView, SAP Business Objects/HANA

There are many difference between processes mining and data mining:
Both start from data, but data mining techniques are not process-centric, they look at isolated
decisions of the type that I've just shown to you.
Topic such as process discovery, conformance checking, and bottleneck analysis are not addressed
by traditional data mining techniques, you need to have process mining for that.
End-to-end process models and concurrency are essential for process mining
Process mining assumes event logs where events have timestamps and refer to cases (process intances)
Process mining and data mining can be combined to answer very advanced questions, so that's important.

           - - - - - - - - - -          - - - - - - - - - - 

Lecture 1.4: Learning Decision Trees (27min)

In a decision tree we have a number of predictor variables, and based on these predictor variables.
We try to predict, what the so-called response variable is.
Decision tree learning is a form of supervised learning, because the data is labeled. Labeled using the
response variable. In our case, this is categorical data.
The is so called a decision tree, it is learned based on the data. and what it tells you
We can use decision tree to make predictions over unseen instances.
Split the set of instances in subsets such that the variation within each subset becomes smaller.
The idea is that we split nodes that we are uncertain about into smaller sets, smaller classes that are
more homogenous
Is crucial for understanding decision trees a good understanding of the notion of entropy.
Entropy is the degree of uncertainty, one can also think of entropy as the inverse of compressibility or
zippability. if there is very little variation within a group, then we can compress the data very much.
The goal is to reduce the entropy in the leaves of the decision tree and in this way improve the 
predictability of elements that belong to a particular class

To formalize the notion of entropy, we need to use logartithms
formula for entropy:
E = - Sum (k, i=1)pi log2(pi)
k possible values enumerated 1, 2, ..., k
pi = ci/n is the fraction of elements having value i
with ci>=1 the number of i values and n = sum(k, i=1)ci
